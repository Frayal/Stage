{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#created the 0/05/2018 14:27: by Alexis Blanchet#\n",
    "#################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy import stats\n",
    "from sklearn.metrics import log_loss\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_annomalies(df,anomalies):\n",
    "    \"\"\"\n",
    "    prend en entrée le nombres d'indicateurs qui detectent une annomalie\n",
    "    ainsi que le datafame contenant tous les features(on a effacé 3 lignes)\n",
    "    renvoie un graph présentant les zones d'incertitudes quand a la présence\n",
    "    d'un événement important dans la plage horaire\n",
    "    en vert une montée d'audience, en orange une baisse d'audiance\n",
    "    \"\"\"\n",
    "    anomalies = list(anomalies)\n",
    "    l1 = find_index(anomalies,0)\n",
    "    l2 = find_index(anomalies,-1)\n",
    "    l3 = find_index(anomalies,1)\n",
    "\n",
    "    x = df['t'].values\n",
    "    t= [i for i in range(len(x))]\n",
    "    x1 = [t[i] for i in l1]\n",
    "    x2 = [t[i] for i in l2]\n",
    "    x3 = [t[i] for i in l3]\n",
    "    y1 = [x[i] for i in l1]\n",
    "    y2 = [x[i] for i in l2]\n",
    "    y3 = [x[i] for i in l3]\n",
    "\n",
    "    \n",
    "    trace1 = go.Scatter(\n",
    "        x=x1,\n",
    "        y=y1,\n",
    "        mode = 'markers',\n",
    "        name = 'regular',\n",
    "    \n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x=x2,\n",
    "        y=y2,\n",
    "        mode = 'markers',\n",
    "        name ='anormal loss',\n",
    "    )\n",
    "    trace3 = go.Scatter(\n",
    "        x=x3,\n",
    "        y=y3,\n",
    "        mode = 'markers',\n",
    "        name = 'anormal gain',\n",
    "    )\n",
    "    trace4 = go.Scatter(\n",
    "        x=0,\n",
    "        y=0,\n",
    "        mode = 'markers',\n",
    "        name = 'begin of programmes',\n",
    "    )\n",
    "        \n",
    "    fig = tools.make_subplots(rows=4, cols=1, specs=[[{}], [{}], [{}], [{}]],\n",
    "                              shared_xaxes=True, shared_yaxes=True,\n",
    "                              vertical_spacing=0.001)\n",
    "    fig.append_trace(trace1, 1, 1)\n",
    "    fig.append_trace(trace2, 1, 1)\n",
    "    fig.append_trace(trace3, 1, 1)\n",
    "    fig.append_trace(trace4, 1, 1)\n",
    "\n",
    "    fig['layout'].update(height=3000, width=2000, title='Annomalie detection')\n",
    "    plot(fig, filename='data.html')\n",
    "\n",
    "def find_index(l,v):\n",
    "    res = []\n",
    "    for i, j in enumerate(l):\n",
    "        if(j == v):\n",
    "            res.append(i)\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/alexis/Bureau/Stage/Time-series/data/processed/sfrdaily_20180430_0_192_0_cleandata-processed.csv')\n",
    "y = pd.read_csv('/home/alexis/Bureau/Stage/Time-series/y_true2.csv')\n",
    "df = df.drop(['label'],axis=1)\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.fillna(1)\n",
    "X_train = df.values\n",
    "y_train = y['CP'][3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "dataset = df\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "Y = y_train.reshape(-1, 1)\n",
    "scalery = MinMaxScaler(feature_range=(0, 1))\n",
    "Y = scalery.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962 475\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "trainY, testY = Y[0:train_size], Y[train_size:len(dataset)]\n",
    "print(len(trainX), len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def f2_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, \"int32\")\n",
    "    y_pred = tf.cast(tf.round(y_pred*2), \"int32\") # implicit 0.5 threshold via tf.round\n",
    "    y_correct = y_true * y_pred\n",
    "    sum_true = tf.reduce_sum(y_true, axis=1)\n",
    "    sum_pred = tf.reduce_sum(y_pred, axis=1)\n",
    "    sum_correct = tf.reduce_sum(y_correct, axis=1)\n",
    "    precision = sum_correct / sum_pred\n",
    "    recall = sum_correct / sum_true\n",
    "    f_score = 2 * precision * recall / (precision + recall)\n",
    "    f_score = tf.where(tf.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
    "    return tf.reduce_mean(f_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numpy.random.seed(99)\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, 27), dropout=0.4))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[f2_score])\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.36 RMSE\n",
      "Test Score: 0.39 RMSE\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[:,0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[:,0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = list([1 if i[0]>0.15 else 0 for i in testPredict])\n",
    "trainPredict = list([1 if i[0]>0.15 else 0 for i in trainPredict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[398, 409, 413, 428, 429, 441, 442, 454, 455, 456, 468, 469, 487, 494, 497, 518, 523, 529, 535, 568, 585, 588, 591, 595, 634, 642, 652, 656, 657, 676, 682, 711, 748, 757, 778, 784, 827, 828, 833, 849, 850, 859, 861, 869, 893, 896, 897, 915, 936, 960, 981, 1008, 1048, 1049, 1060, 1061, 1067, 1071, 1078, 1079, 1086, 1087, 1120, 1155, 1156, 1187, 1188, 1189, 1190, 1215, 1250, 1278, 1290, 1305, 1333, 1344, 1368, 1380]\n",
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]\n",
      "[ (2,1) x1,y2 ]\n",
      "[ (3,1) x1,y3 ]\n",
      "[ (4,1) x1,y4 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file:///home/alexis/Bureau/Stage/ML/data.html'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df['t'].values\n",
    "t= [i for i in range(len(x))]\n",
    "\n",
    "\n",
    "l1 = find_index(trainPredict,1)\n",
    "l2 = find_index(testPredict,1)\n",
    "\n",
    "x1 = [t[i] for i in l1]\n",
    "x2 = [t[i+train_size] for i in l2]\n",
    "\n",
    "y1 = [x[i] for i in l1]\n",
    "y2 = [x[i+train_size] for i in l2]\n",
    "\n",
    "l3 = find_index(y_train,1)\n",
    "print(l3)\n",
    "x3 = [t[i] for i in l3]\n",
    "y3 = [x[i] for i in l3]\n",
    "\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "        x= t,\n",
    "        y= x,\n",
    "        name = 'true',\n",
    "    \n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "        x =x1,\n",
    "        y=y1,\n",
    "        mode = 'markers',\n",
    "        name ='train',\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "        x=x2,\n",
    "        y= y2,\n",
    "        mode = 'markers',\n",
    "        name = 'test',\n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "        x=x3,\n",
    "        y=y3,\n",
    "        mode = 'markers',\n",
    "        name = 'begin of programmes',\n",
    ")\n",
    "        \n",
    "fig = tools.make_subplots(rows=4, cols=1, specs=[[{}], [{}], [{}], [{}]],\n",
    "                              shared_xaxes=True, shared_yaxes=True,\n",
    "                              vertical_spacing=0.001)\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 1)\n",
    "fig.append_trace(trace3, 1, 1)\n",
    "fig.append_trace(trace4, 1, 1)\n",
    "\n",
    "fig['layout'].update(height=3000, width=2000, title='Annomalie detection')\n",
    "plot(fig, filename='data.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/home/alexis/Bureau/Stage/Time-series/data/processed/sfrdaily_20171215_0_192_0_cleandata-processed.csv')\n",
    "df2 = df2.drop(['label'],axis=1).fillna(1)\n",
    "X = scaler.fit_transform(df2)\n",
    "X = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]\n",
      "[ (2,1) x1,y2 ]\n",
      "[ (3,1) x1,y3 ]\n",
      "[ (4,1) x1,y4 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file:///home/alexis/Bureau/Stage/ML/data.html'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = [1 if x[0]>0.17 else 0 for x in pred]\n",
    "x = df2['t'].values\n",
    "t= [i for i in range(len(x))]\n",
    "\n",
    "\n",
    "l1 = find_index(v,1)\n",
    "\n",
    "x1 = [t[i] for i in l1]\n",
    "y1 = [x[i] for i in l1]\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "        x= t,\n",
    "        y= x,\n",
    "        name = 'true',\n",
    "    \n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "        x =x1,\n",
    "        y=y1,\n",
    "        mode = 'markers',\n",
    "        name ='train',\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "        x=0,\n",
    "        y= 0,\n",
    "        mode = 'markers',\n",
    "        name = 'test',\n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "        x=0,\n",
    "        y=0,\n",
    "        mode = 'markers',\n",
    "        name = 'begin of programmes',\n",
    ")\n",
    "        \n",
    "fig = tools.make_subplots(rows=4, cols=1, specs=[[{}], [{}], [{}], [{}]],\n",
    "                              shared_xaxes=True, shared_yaxes=True,\n",
    "                              vertical_spacing=0.001)\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 1)\n",
    "#fig.append_trace(trace3, 1, 1)\n",
    "#fig.append_trace(trace4, 1, 1)\n",
    "\n",
    "fig['layout'].update(height=3000, width=2000, title='Annomalie detection')\n",
    "plot(fig, filename='data.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
